<!-- Generated by SpecStory -->

# Server Code Analysis and Documentation (2025-03-21 12:30:30)

_**User**_

@server.ts can you please analyze the code of server.ts, and then document your findings in the @SERVER_ARCHITECTURE.md file using markdown syntax.

Be detailed and thorough, list what it does, how it does it (and what packages it uses) and what features and functions it provides.

---

_**Assistant**_

I'll analyze the server code and create comprehensive documentation. First, I'll read the server code and then create the architecture documentation.


---

```diff
- 
+ # Server Architecture Documentation
+ 
+ ## Overview
+ The server is a Node.js/Express application that provides a WebSocket-based API for interacting with OpenAI's GPT models. It implements rate limiting, Firebase integration for data persistence, and real-time streaming of AI responses.
+ 
+ ## Core Technologies
+ - **Express.js**: Web application framework
+ - **WebSocket (ws)**: Real-time bidirectional communication
+ - **Firebase Admin SDK**: Database and authentication
+ - **OpenAI API**: AI model integration
+ - **Express Rate Limit**: Request rate limiting
+ - **CORS**: Cross-origin resource sharing
+ - **dotenv**: Environment variable management
+ 
+ ## Key Features
+ 
+ ### 1. Rate Limiting
+ - Implements per-model rate limiting using `express-rate-limit`
+ - Limits are configured per model:
+   - GPT-3.5: 5 requests per 24 hours
+   - GPT-4: 0 requests per 24 hours (effectively disabled)
+ - Uses in-memory storage for rate limit tracking
+ - Custom key generation based on model and fingerprint
+ 
+ ### 2. WebSocket Communication
+ - Provides real-time streaming of AI responses
+ - Handles WebSocket upgrades from HTTP connections
+ - Implements message handling and error management
+ - Streams OpenAI responses chunk by chunk to clients
+ 
+ ### 3. Firebase Integration
+ - Stores completion data in Firestore
+ - Uses Firebase Admin SDK for server-side authentication
+ - Maintains a collection of "completions" with timestamps
+ 
+ ### 4. API Endpoints
+ 
+ #### REST Endpoints
+ - `/api/completion`: Basic prompt handling endpoint
+ - `/api/prompts-remaining`: Check remaining rate limit tokens
+ - `/api/moar-prompts`: Decrement rate limit counter (admin function)
+ - `/api/use-prompt`: Increment rate limit counter
+ - `/api/examples`: Retrieve example prompts from filesystem
+ - `/api/hello`: Health check endpoint
+ 
+ #### WebSocket Endpoint
+ - `/ws`: WebSocket connection endpoint for real-time AI interactions
+ 
+ ## Data Flow
+ 
+ 1. **Client Connection**:
+    - Client establishes WebSocket connection
+    - Server handles upgrade from HTTP to WebSocket
+ 
+ 2. **Message Processing**:
+    - Client sends message with prompt and parameters
+    - Server validates rate limits
+    - Server stores request in Firebase
+    - Server initiates OpenAI API call
+ 
+ 3. **Response Streaming**:
+    - OpenAI responses are streamed in real-time
+    - Server processes chunks and forwards to client
+    - Client receives incremental updates
+ 
+ ## Security Features
+ - CORS enabled for cross-origin requests
+ - Rate limiting to prevent abuse
+ - Firebase authentication for database access
+ - Environment variable configuration for sensitive data
+ 
+ ## Configuration
+ - Port: Configurable via environment variable (default: 6823)
+ - OpenAI API key: Required environment variable
+ - Firebase credentials: Required environment variables
+   - Project ID
+   - Private Key
+   - Client Email
+ 
+ ## Error Handling
+ - WebSocket error handling
+ - OpenAI API error handling
+ - Firebase operation error handling
+ - Rate limit exceeded notifications
+ 
+ ## Current Limitations
+ - Rate limiting uses in-memory storage (resets on server restart)
+ - No persistent user authentication
+ - Limited error recovery mechanisms
+ - No request validation middleware
+ 
+ ## TypeScript Implementation Notes
+ - Uses TypeScript for type safety
+ - Some type issues present in WebSocket streaming implementation
+ - Implicit 'any' types in some callback parameters
+ 
+ ## Dependencies
+ ```json
+ {
+   "express": "Web framework",
+   "ws": "WebSocket implementation",
+   "firebase-admin": "Firebase integration",
+   "openai": "OpenAI API client",
+   "express-rate-limit": "Rate limiting",
+   "cors": "Cross-origin support",
+   "dotenv": "Environment management"
+ }
+ ```
+ 
```

---

I've created a comprehensive architecture document that covers all aspects of the server implementation. The documentation includes:

1. A high-level overview of the server's purpose
2. Core technologies and dependencies used
3. Detailed breakdown of key features including rate limiting, WebSocket communication, and Firebase integration
4. Complete API endpoint documentation
5. Data flow explanation
6. Security features and configuration requirements
7. Error handling mechanisms
8. Current limitations and TypeScript implementation notes

The documentation should help developers understand:
- How the server works
- What features are available
- How to interact with the API
- Current limitations and potential areas for improvement

Would you like me to expand on any particular section or add more specific details about any component?

---



---

